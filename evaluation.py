{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f09a4a45-4153-40a7-ae23-095695855bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dhc/home/sandro.steinwand/conda3/envs/gpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import sklearn.metrics\n",
    "from skweak import utils\n",
    "from spacy.tokens import Span # type: ignore\n",
    "\n",
    "def evaluate(docs, all_labels, target_sources):\n",
    "    \"\"\"Extracts the evaluation results for one or more sources, and add them to a pandas DataFrame.\"\"\"\n",
    "    \n",
    "    if isinstance(target_sources, str):\n",
    "        target_sources = [target_sources]\n",
    "\n",
    "    records = []\n",
    "    for source in target_sources:\n",
    "        results = get_results(docs, all_labels, source)\n",
    "        labels = set(results[\"label_weights\"].keys())\n",
    "        for name in sorted(labels) + [\"micro\", \"weighted\", \"macro\"]:\n",
    "            if name in results:\n",
    "                record = results[name]\n",
    "                record[\"label\"] = name\n",
    "                record[\"model\"] = source\n",
    "                if name in labels:\n",
    "                    record[\"proportion\"] = results[\"label_weights\"][name]          \n",
    "                records.append(record)\n",
    "    \n",
    "    df = pandas.DataFrame.from_records(records)\n",
    "    df[\"proportion\"] = df.proportion.apply(lambda x: \"%.1f %%\"%(x*100) if not np.isnan(x) else \"\")\n",
    "    df[\"tok_cee\"] = df.tok_cee.apply(lambda x: str(x) if not np.isnan(x) else \"\")\n",
    "    df[\"tok_acc\"] = df.tok_acc.apply(lambda x: str(x) if not np.isnan(x) else \"\")\n",
    "    df[\"coverage\"] = df.coverage.apply(lambda x: str(x) if not np.isnan(x) else \"\")\n",
    "    df = df.set_index([\"label\", \"proportion\", \"model\"]).sort_index()\n",
    "    df = df[[\"tok_precision\", \"tok_recall\", \"tok_f1\", \"tok_cee\", \"tok_acc\", \"coverage\",\n",
    "             \"ent_precision\", \"ent_recall\", \"ent_f1\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_results(docs, all_labels, target_source, conf_threshold=0.5):\n",
    "    \"\"\"Computes the usual metrics (precision, recall, F1, cross-entropy) on the dataset, using the spacy entities \n",
    "    in each document as gold standard, and the annotations of a given source as the predicted values\"\"\"\n",
    "\n",
    "\n",
    "    all_numbers = compute_raw_numbers(docs, all_labels, target_source, conf_threshold)\n",
    "    tok_tp, tok_fp, tok_fn, tok_logloss, tok_nb, tok_tp_tn, ent_tp, ent_fp, ent_fn, ent_support, tok_support = all_numbers\n",
    "\n",
    "    # We then compute the metrics themselves\n",
    "    results = {}\n",
    "    for label in ent_support:\n",
    "        ent_pred = ent_tp[label]+ent_fp[label] + 1E-10\n",
    "        ent_true = ent_tp[label]+ent_fn[label] + 1E-10\n",
    "        tok_pred = tok_tp[label]+tok_fp[label] + 1E-10\n",
    "        tok_true = tok_tp[label]+tok_fn[label] + 1E-10\n",
    "        results[label] = {}\n",
    "        results[label][\"ent_precision\"] = round(ent_tp[label]/ent_pred, 3)\n",
    "        results[label][\"ent_recall\"] = round(ent_tp[label]/ent_true, 3)\n",
    "        results[label][\"tok_precision\"] = round(tok_tp[label]/tok_pred, 3)\n",
    "        results[label][\"tok_recall\"] = round(tok_tp[label]/tok_true, 3)\n",
    "        \n",
    "        ent_f1_numerator = (results[label][\"ent_precision\"] * results[label][\"ent_recall\"])\n",
    "        ent_f1_denominator = (results[label][\"ent_precision\"] +results[label][\"ent_recall\"]) + 1E-10\n",
    "        results[label][\"ent_f1\"] = 2*round(ent_f1_numerator / ent_f1_denominator, 3)\n",
    "            \n",
    "        tok_f1_numerator = (results[label][\"tok_precision\"] * results[label][\"tok_recall\"])\n",
    "        tok_f1_denominator = (results[label][\"tok_precision\"] +results[label][\"tok_recall\"]) + 1E-10\n",
    "        results[label][\"tok_f1\"] = 2*round(tok_f1_numerator / tok_f1_denominator, 3)\n",
    "    \n",
    "    results[\"macro\"] = {\"ent_precision\":round(np.mean([results[l][\"ent_precision\"] for l in ent_support]), 3), \n",
    "                       \"ent_recall\":round(np.mean([results[l][\"ent_recall\"] for l in ent_support]), 3), \n",
    "                       \"tok_precision\":round(np.mean([results[l][\"tok_precision\"] for l in ent_support]), 3), \n",
    "                       \"tok_recall\":round(np.mean([results[l][\"tok_recall\"] for l in ent_support]), 3)}\n",
    "    \n",
    "        \n",
    "    label_weights = {l:ent_support[l]/sum(ent_support.values()) for l in ent_support}\n",
    "    results[\"label_weights\"] = label_weights\n",
    "    results[\"weighted\"] = {\"ent_precision\":round(np.sum([results[l][\"ent_precision\"]*label_weights[l] \n",
    "                                                            for l in ent_support]), 3), \n",
    "                           \"ent_recall\":round(np.sum([results[l][\"ent_recall\"]*label_weights[l] \n",
    "                                                         for l in ent_support]), 3), \n",
    "                           \"tok_precision\":round(np.sum([results[l][\"tok_precision\"]*label_weights[l] \n",
    "                                                           for l in ent_support]), 3), \n",
    "                           \"tok_recall\":round(np.sum([results[l][\"tok_recall\"]*label_weights[l] \n",
    "                                                        for l in ent_support]), 3)}\n",
    "    \n",
    "    ent_pred = sum([ent_tp[l] for l in ent_support]) + sum([ent_fp[l] for l in ent_support]) + 1E-10\n",
    "    ent_true = sum([ent_tp[l] for l in ent_support]) + sum([ent_fn[l] for l in ent_support]) + 1E-10\n",
    "    tok_pred = sum([tok_tp[l] for l in ent_support]) + sum([tok_fp[l] for l in ent_support]) + 1E-10\n",
    "    tok_true = sum([tok_tp[l] for l in ent_support])  + sum([tok_fn[l] for l in ent_support]) + 1E-10\n",
    "    results[\"micro\"] = {\"ent_precision\":round(sum([ent_tp[l] for l in ent_support]) / ent_pred, 3), \n",
    "                        \"ent_recall\":round(sum([ent_tp[l] for l in ent_support]) / ent_true, 3), \n",
    "                        \"tok_precision\":round(sum([tok_tp[l] for l in ent_support]) /tok_pred, 3), \n",
    "                        \"tok_recall\":round(sum([tok_tp[l] for l in ent_support]) / tok_true, 3),\n",
    "                        \"tok_cee\":round(tok_logloss/tok_nb, 3),\n",
    "                        \"tok_acc\": round(tok_tp_tn/tok_nb, 3),\n",
    "                        \"coverage\":round((sum(tok_tp.values()) +sum(tok_fp.values())) / sum(tok_support.values()), 3)}\n",
    "    \n",
    "    for metric in [\"macro\", \"weighted\", \"micro\"]:\n",
    "        ent_f1_numerator = (results[metric][\"ent_precision\"] * results[metric][\"ent_recall\"])\n",
    "        ent_f1_denominator = (results[metric][\"ent_precision\"] +results[metric][\"ent_recall\"]) + 1E-10\n",
    "        results[metric][\"ent_f1\"] = 2*round(ent_f1_numerator / ent_f1_denominator, 3)\n",
    "            \n",
    "        tok_f1_numerator = (results[metric][\"tok_precision\"] * results[metric][\"tok_recall\"])\n",
    "        tok_f1_denominator = (results[metric][\"tok_precision\"] +results[metric][\"tok_recall\"]) + 1E-10\n",
    "        results[metric][\"tok_f1\"] = 2*round(tok_f1_numerator / tok_f1_denominator, 3)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_raw_numbers(docs, all_labels, target_source, conf_threshold=0.5):\n",
    "    \"\"\"Computes the raw metrics (true positives, true negatives, ...) on the dataset, using the spacy entities \n",
    "    in each document as gold standard, and the annotations of a given source as the predicted values\"\"\"\n",
    "\n",
    "    # We start by computing the TP, FP and FN values\n",
    "    tok_tp = {}\n",
    "    tok_fp = {}\n",
    "    tok_fn ={}\n",
    "\n",
    "    tok_logloss = 0\n",
    "    tok_nb = 0\n",
    "    tok_tp_tn = 0\n",
    "    \n",
    "    ent_tp ={}\n",
    "    ent_fp = {}\n",
    "    ent_fn = {}\n",
    "    ent_support = {}\n",
    "    tok_support = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        if target_source in doc.spans:\n",
    "            spans = utils.get_spans_with_probs(doc, target_source)\n",
    "        else:\n",
    "            spans = []\n",
    "        spans = [span for (span, prob) in spans if prob >= conf_threshold]\n",
    "            \n",
    "        for label in all_labels:\n",
    "            true_spans = {(ent.start, ent.end) for ent in doc.ents if ent.label_==label}\n",
    "            pred_spans = {(span.start,span.end) for span in spans if span.label_==label}\n",
    "        \n",
    "            ent_tp[label] = ent_tp.get(label,0) + len(true_spans.intersection(pred_spans))\n",
    "            ent_fp[label] = ent_fp.get(label,0) + len(pred_spans - true_spans)\n",
    "            ent_fn[label] = ent_fn.get(label,0) +  len(true_spans - pred_spans)\n",
    "            ent_support[label] = ent_support.get(label, 0) + len(true_spans)\n",
    "            \n",
    "            true_tok_labels = {i for start,end in true_spans for i in range(start, end)}\n",
    "            pred_tok_labels = {i for start,end in pred_spans for i in range(start, end)}\n",
    "            tok_tp[label] = tok_tp.get(label, 0) + len(true_tok_labels.intersection(pred_tok_labels))\n",
    "            tok_fp[label] = tok_fp.get(label, 0) + len(pred_tok_labels - true_tok_labels)\n",
    "            tok_fn[label] = tok_fn.get(label,0) + len(true_tok_labels - pred_tok_labels)\n",
    "            tok_support[label] = tok_support.get(label, 0) + len(true_tok_labels)\n",
    "\n",
    "        gold_probs, pred_probs = _get_probs(doc, all_labels, target_source)\n",
    "        tok_logloss += sklearn.metrics.log_loss(gold_probs, pred_probs, normalize=False)\n",
    "        tok_tp_tn += sum(gold_probs.argmax(axis=1) == pred_probs.argmax(axis=1))\n",
    "        tok_nb += len(doc)\n",
    "\n",
    "    return (tok_tp, tok_fp, tok_fn, tok_logloss, tok_nb, tok_tp_tn, ent_tp, \n",
    "            ent_fp, ent_fn, ent_support, tok_support)\n",
    "\n",
    "\n",
    "def _get_probs(doc, all_labels, target_source):\n",
    "    \"\"\"Retrieves the gold and predicted probabilities (as matrices)\"\"\"\n",
    "    \n",
    "    out_label_indices = {\"O\":0}\n",
    "    for label in all_labels:\n",
    "        for prefix in \"BI\":\n",
    "            out_label_indices[\"%s-%s\" % (prefix, label)] = len(out_label_indices)\n",
    "                            \n",
    "    gold_probs = np.zeros((len(doc), len(out_label_indices)), dtype=np.int16)    \n",
    "    for ent in doc.ents:\n",
    "        gold_probs[ent.start, out_label_indices.get(\"B-%s\" % ent.label_, 0)] = 1\n",
    "        for i in range(ent.start+1, ent.end):\n",
    "            gold_probs[i, out_label_indices.get(\"I-%s\" % ent.label_, 0)] = 1\n",
    "    \n",
    "    pred_probs = np.zeros(gold_probs.shape)\n",
    "    if target_source in doc.spans and \"probs\" in doc.spans[target_source].attrs:       \n",
    "        for tok_pos, labels in doc.spans[target_source].attrs[\"probs\"].items():\n",
    "            for label, label_prob in labels.items():\n",
    "                pred_probs[tok_pos, out_label_indices[label]] = label_prob\n",
    "        pred_probs[:,0] = np.clip(1-pred_probs[:,1:].sum(axis=1), 0.0, 1.0)\n",
    "    else:\n",
    "        vector = utils.spans_to_array(doc, all_labels, [target_source])[:,0]\n",
    "        pred_probs[np.arange(vector.size), vector] = True       \n",
    "    \n",
    "    return gold_probs, pred_probs\n",
    "\n",
    "\n",
    "def show_errors(docs, all_labels, target_source, conf_threshold=0.5):\n",
    "    \"\"\"Utilities to display the errors/omissions of a given source\"\"\"\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        \n",
    "        spans = utils.get_spans_with_probs(doc, target_source)\n",
    "            \n",
    "        print(\"Doc %i:\"%i, doc)\n",
    "        true_spans = {(ent.start, ent.end):ent.label_ for ent in doc.ents}\n",
    "        pred_spans = {(span.start,span.end):span.label_ for span, prob in spans if prob >=conf_threshold}\n",
    "\n",
    "        for start,end in true_spans:\n",
    "            if (start,end) not in pred_spans:\n",
    "                print(\"Not found: %s [%i:%i] -> %s\"%(doc[start:end], start, end, true_spans[(start,end)]))\n",
    "            elif true_spans[(start,end)]!=pred_spans[(start,end)]:\n",
    "                print(\"Wrong label: %s [%i:%i] -> %s but predicted as %s\"%(doc[start:end], start, end, \n",
    "                true_spans[(start,end)], pred_spans[(start,end)]))\n",
    "\n",
    "        for start,end in pred_spans:\n",
    "            if (start,end) not in true_spans:\n",
    "                print(\"Spurious: %s [%i:%i] -> %s\"%(doc[start:end], start, end, pred_spans[(start,end)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c84a9-2bce-4acf-b84a-e4de92c66259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu] *",
   "language": "python",
   "name": "conda-env-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
